{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Detecting the fiducial particles in LM images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiducial particles/markers are used in correlated light and electron microscopy (CLEM) to enable accurate overlaying of fluorescence (LM) and electron microscopy (EM) images. The fiducial particles in LM images appear as bright spots. \n",
    "\n",
    "In this notebook, we **detect fiducial particles** in **LM images** using Big-FISH python package originally created for analysis of smFISH images. It includes various methods, among them also the spot detection method which we are using in this notebook. The main steps of this algorithm then are:\n",
    "- **1. Spot detection** - Detection of fiducial particles using spot detection methods from Big-FISH package\n",
    "- **2. Dense region decomposition** - Recognizing larger and brighter (dense) spots as clusters and estimating the number of individual fiducial particles building these clusters\n",
    "- **3. Cluster detection** - Detecting the position of larger and brighter (dense) spots\n",
    "- **4. Results saving** - Saving the positions of all detected fiducial particles and the positions of fiducial clusters into files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T16:01:29.143710Z",
     "start_time": "2022-09-05T16:01:27.173886Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import plot_image, list_to_dataframe, dataframe_to_xml, dataframe_to_csv, dataframe_to_pointcloud\n",
    "import open3d as o3d\n",
    "from probreg import cpd\n",
    "from pathlib import Path\n",
    "import bigfish\n",
    "import bigfish.stack as stack\n",
    "import bigfish.detection as detection\n",
    "import bigfish.multistack as multistack\n",
    "import bigfish.plot as plot\n",
    "print(\"Big-FISH version: {0}\".format(bigfish.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to the input EM and LM images and load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T16:01:29.258259Z",
     "start_time": "2022-09-05T16:01:29.242094Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "input_folder = 'E:/DATA/AI4Life_Pr26/20240805_Trial_data_fiducial_particles/240723_JB294_CLEM-AI4life_sample1/pos1'\n",
    "test_folder = '//vironova.com/root/Users/kristinal/Documents/1Test'\n",
    "\n",
    "EM_image_path = os.path.join(input_folder, \"240726_JB295_HEK293_CLEM_LAMP1-488_Particles-555_grid4_pos1_bin4_EM.tif\")\n",
    "LM_image_path = os.path.join(input_folder, \"240726_JB295_HEK293_CLEM_LAMP1-488_Particles-555_grid4_pos1_LM.tif\")\n",
    "\n",
    "print(Path(EM_image_path).exists())\n",
    "print(Path(LM_image_path).exists())\n",
    "\n",
    "output_folder = Path(os.path.join(input_folder,\"output\"))\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "\n",
    "EMimage = stack.read_image(EM_image_path)\n",
    "LMimage3 = stack.read_image(LM_image_path)     # 3 channel image\n",
    "LMimage = LMimage3[:,:,1]                      # 1 channel image - only the channel with the fiducial particles\n",
    "\n",
    "large_LMimage3 = stack.resize_image(LMimage3, EMimage.shape, method='bilinear')\n",
    "large_LMimage = large_LMimage3[:,:,1]\n",
    "\n",
    "scale_x = EMimage.shape[0]/LMimage.shape[0]\n",
    "scale_y = EMimage.shape[1]/LMimage.shape[1]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "print(\"EMimage\")\n",
    "print(\"\\r shape: {0}\".format(EMimage.shape))\n",
    "print(\"\\r dtype: {0}\".format(EMimage.dtype))\n",
    "\n",
    "print(\"LMimage:\")\n",
    "print(\"\\r shape: {0}\".format(LMimage3.shape))\n",
    "print(\"\\r dtype: {0}\".format(LMimage3.dtype))\n",
    "\n",
    "print(\"resized_LMimage:\")\n",
    "print(\"\\r shape: {0}\".format(large_LMimage.shape))\n",
    "print(\"\\r dtype: {0}\".format(large_LMimage.dtype))\n",
    "\n",
    "print(\"Scale in X direction: \", scale_x)\n",
    "print(\"Scale in X direction: \", scale_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the EM image overlaid with the LM image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,30))\n",
    "plt.imshow(large_LMimage, cmap='gray', alpha=0.5)\n",
    "plt.imshow(EMimage, cmap='gray', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1. Spot detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We assume **spot is a local maximum** in the LM image. Three steps are required to detect them:\n",
    "- Filter the LM image to enhance the signal-to-noise ratio and denoise the image (`bigfish.stack.log_filter`).\n",
    "- Detect the local maximum in the filtered image (`bigfish.detection.local_maximum_detection`).\n",
    "- Remove the local maximum under a fixed threshold (`bigfish.detection.spots_thresholding`). To be robust, the thresholding should be applied on the filtered image. Thus, the threshold is set relatively to the filtered image values.\n",
    "- If necessary, the optimal threshold can be estimated with `bigfish.detection.automated_threshold_setting` (applied on a filtered image).\n",
    "\n",
    "All these steps are summarized in `bigfish.detection.detect_spots` that return the 2D coordinates of the detected spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T16:01:31.903304Z",
     "start_time": "2022-09-05T16:01:29.274904Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "vox_size = 110  # voxel size in nanometers\n",
    "spot_rad = 110 \n",
    "\n",
    "spots, threshold = detection.detect_spots(\n",
    "    images=LMimage, \n",
    "    return_threshold=True, \n",
    "    voxel_size=(vox_size, vox_size),  # in nanometer (one value per dimension zyx)\n",
    "    spot_radius=(spot_rad, spot_rad))  # in nanometer (one value per dimension zyx)\n",
    "\n",
    "# From Fiji file:\n",
    "# Width:  29.2602 microns (266)\n",
    "# Height:  71.2805 microns (648)\n",
    "# Size:  1010K\n",
    "# Resolution:  9.0908 pixels per micron\n",
    "# Voxel size: 0.1100x0.1100x1 micron^3\n",
    "\n",
    "print(\"detected spots\")\n",
    "print(\"\\r shape: {0}\".format(spots.shape))\n",
    "print(\"\\r dtype: {0}\".format(spots.dtype))\n",
    "print(\"\\r threshold: {0}\".format(threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Given the **voxel size** and the expected **spot radius** (in nanometer), the function `bigfish.detection.detect_spots` automatically estimates a **kernel size** for the LoG filtering and a **minimal distance** between two spots we want to be able to detect separately. It is still possible to set these parameters explicitly in order to fine-tune the detection. Internally, we approximate them as the spot radius in pixel with the function `bigfish.detection.get_object_radius_pixel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__Note:__ What we call spot radius in this notebook can be understood as its **Point Spread Function (PSF)**. For simplicity sake, this PSF is modelled as a 2D gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T16:01:35.746683Z",
     "start_time": "2022-09-05T16:01:34.521143Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# spot radius\n",
    "spot_radius_px = detection.get_object_radius_pixel(\n",
    "    voxel_size_nm=(vox_size, vox_size), \n",
    "    object_radius_nm=(spot_rad, spot_rad), \n",
    "    ndim=2)\n",
    "\n",
    "# LoG filter\n",
    "rna_log = stack.log_filter(LMimage, sigma=spot_radius_px)\n",
    "\n",
    "# local maximum detection\n",
    "mask = detection.local_maximum_detection(rna_log, min_distance=spot_radius_px)\n",
    "\n",
    "# thresholding\n",
    "threshold = detection.automated_threshold_setting(rna_log, mask)\n",
    "threshold = 1500\n",
    "spots, _ = detection.spots_thresholding(rna_log, mask, threshold)\n",
    "\n",
    "print(\"detected spots\")\n",
    "print(\"\\r shape: {0}\".format(spots.shape))\n",
    "print(\"\\r dtype: {0}\".format(spots.dtype))\n",
    "print(\"\\r threshold: {0}\".format(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T16:01:38.786387Z",
     "start_time": "2022-09-05T16:01:35.749355Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plot.plot_detection(LMimage, spots, contrast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 2. Dense region decomposition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The detection of local maximum is not able to detect individual fiducial particles clustered in a dense and bright region. We try to **decompose these regions by simulating as many spots as possible until we match the original region intensity**. Our current steps are:\n",
    "- Denoise the LM image by estimating then removing its background (`bigfish.stack.remove_background_gaussian`).\n",
    "- Build a reference median spot from the individual predetected spots (`bigfish.detection.build_reference_spot`).\n",
    "- Fit a gaussian signal on the reference spot (`bigfish.detection.modelize_spot`).\n",
    "- Detect the candidate dense regions in the denoised image - large regions brighter than the reference spot (`bigfish.detection.get_dense_region`).\n",
    "- Use the fitted gaussian signal to fill as many spots in the candidate regions as possible (`bigfish.detection.simulate_gaussian_mixture`).\n",
    "\n",
    "All these steps are summarized in `bigfish.detection.decompose_dense` that return the coordinates of the detected spots outside and inside a decomposed region, additional information about the regions themself and an image of the reference spot estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T16:01:42.141254Z",
     "start_time": "2022-09-05T16:01:41.134544Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "spots_post_decomposition, dense_regions, reference_spot = detection.decompose_dense(\n",
    "    image=LMimage, \n",
    "    spots=spots, \n",
    "    voxel_size=(vox_size, vox_size), \n",
    "    spot_radius=(spot_rad, spot_rad), \n",
    "    alpha=0.25,  # alpha impacts the number of spots per candidate region\n",
    "    beta=1,  # beta impacts the number of candidate regions to decompose\n",
    "    gamma=5)  # gamma the filtering step to denoise the image\n",
    "\n",
    "#spots_post_decomposition = np.unique((spots_post_decomposition), axis=0)    # remove duplicates\n",
    "\n",
    "print(\"detected spots before decomposition\")\n",
    "print(\"\\r shape: {0}\".format(spots.shape))\n",
    "print(\"\\r dtype: {0}\".format(spots.dtype), \"\\n\")\n",
    "print(\"detected spots after decomposition\")\n",
    "print(\"\\r shape: {0}\".format(spots_post_decomposition.shape))\n",
    "print(\"\\r dtype: {0}\".format(spots_post_decomposition.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T16:01:47.374203Z",
     "start_time": "2022-09-05T16:01:43.087698Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plot.plot_detection(LMimage, spots_post_decomposition, contrast=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 3. Clusters detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Two spots are considered connected if they localized within a specific radius (in nanometer). Above a minimum number of connected spots, a cluster can be defined. This detection can be computed with `bigfish.detection.detect_clusters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T16:01:47.529239Z",
     "start_time": "2022-09-05T16:01:47.519310Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "spots_post_clustering, clusters = detection.detect_clusters(\n",
    "    spots=np.asarray(spots_post_decomposition), \n",
    "    voxel_size=(110, 110), \n",
    "    radius=110, \n",
    "    nb_min_spots=3)\n",
    "\n",
    "#spots_post_clustering = np.unique((spots_post_clustering), axis=0)    # remove duplicates\n",
    "#clusters = np.unique((clusters), axis=0)    # remove duplicates\n",
    "\n",
    "print(\"detected spots after clustering\")\n",
    "print(\"\\r shape: {0}\".format(spots_post_clustering.shape))\n",
    "print(\"\\r dtype: {0}\".format(spots_post_clustering.dtype), \"\\n\")\n",
    "print(\"detected clusters\")\n",
    "print(\"\\r shape: {0}\".format(clusters.shape))\n",
    "print(\"\\r dtype: {0}\".format(clusters.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T16:01:51.901592Z",
     "start_time": "2022-09-05T16:01:47.531620Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plot.plot_detection(LMimage, \n",
    "                    spots=[spots_post_decomposition, clusters[:, :2]], \n",
    "                    shape=[\"circle\", \"circle\"], \n",
    "                    radius=[3, 6], \n",
    "                    color=[\"red\", \"green\"],\n",
    "                    linewidth=[1, 2], \n",
    "                    fill=[False, False], \n",
    "                    contrast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the LM image upscaled to the size of EM image. First save the spot locations into a pandas dataframe with columns: 'id', 'name', 'pos_x', 'pos_y'. Then convert dataframe into numpy array of coordinate pairs (X,Y), XML file and PLY file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.save_image(large_LMimage3, os.path.join(output_folder, \"resized_LMimage.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all detected fiducial particles into a Pandas dataframe and export it to different file formats\n",
    "def save_spots_to_diffent_files (spots, folder, file_name, scale):    \n",
    "    spots_df = list_to_dataframe(spots, os.path.join(folder, f\"{file_name}_df.csv\"))                                                     #str(output_folder/\"source_all_df.csv\")\n",
    "\n",
    "    dataframe_to_xml(spots_df, os.path.join(folder, f\"{file_name}.xml\"))                      # import file for ICY ec-CLEM plugin\n",
    "    dataframe_to_xml(spots_df, os.path.join(folder, f\"{file_name}_scaled.xml\"), scale)                      # import file for ICY ec-CLEM plugin\n",
    "    dataframe_to_pointcloud(spots_df, os.path.join(folder, f\"{file_name}.ply\"))                      # import file for point cloud registration using Probgreg package in Python\n",
    "    dataframe_to_csv(spots_df, os.path.join(folder, f\"{file_name}.csv\"))                             # import file for BigWarp ImageJ plugin\n",
    "    return spots_df\n",
    "\n",
    "#df = save_spots_to_diffent_files(np.unique((spots_post_decomposition), axis=0), output_folder, \"source_all\", [scale_y, scale_x])  # y and x are swapped in the dataframe\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the fiducial particles, regions and clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the detected spots 'spots' as representants for regions\n",
    "df = save_spots_to_diffent_files(np.unique((spots), axis=0), output_folder, \"source_regions\", [scale_y, scale_x])  # y and x are swapped in the dataframe\n",
    "print(df)\n",
    "\n",
    "# Save the 'spots_post_decomposition' as positions for all fiducial particles\n",
    "df = save_spots_to_diffent_files(np.unique((spots_post_decomposition), axis=0), output_folder, \"source_all\", [scale_y, scale_x])  # y and x are swapped in the dataframe\n",
    "print(df)\n",
    "\n",
    "# Save the 'clusters'\n",
    "df = save_spots_to_diffent_files(np.unique((clusters[:, :2]), axis=0), output_folder, \"source_clusters\", [scale_y, scale_x])  # y and x are swapped in the dataframe\n",
    "print(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenation of the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.filters as filters\n",
    "import skimage.morphology as morphology\n",
    "\n",
    "# thresholding\n",
    "threshold = filters.threshold_otsu(large_LMimage)\n",
    "print(threshold)\n",
    "img_thresholded = large_LMimage >= 25000\n",
    "\n",
    "# filter by size\n",
    "#img_filtered = morphology.remove_small_objects(img_thresholded, min_size=100)\n",
    "#img_filtered = morphology.remove_small_holes(img_filtered)\n",
    "#img_filtered = morphology.binary_opening(img_filtered, morphology.disk(2))  # opening and closing maybe not needed since it smooths the protrusions, \n",
    "#img_filtered = morphology.binary_closing(img_filtered, morphology.disk(2))  # but at the moment here to keep it as usefull code\n",
    "#img_filtered = img_thresholded\n",
    "\n",
    "# plot pre-processing\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.imshow(img_thresholded,cmap=\"gray\")\n",
    "ax1.set_title('Original')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_detection(img_thresholded*255, spots_post_decomposition*[scale_x, scale_y], contrast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.imshow(img_thresholded , cmap='gray', alpha=0.5)\n",
    "plt.imshow(EMimage, cmap='gray', alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI4Life_OC2_2024_26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
