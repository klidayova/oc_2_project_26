{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bd973a",
   "metadata": {},
   "source": [
    "This notebook aims to test different registration methods using the Ground Truth points that were manually detected on EM and LM images.\n",
    "\n",
    "#### Loading the Ground truth files in XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70880430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import bigfish.stack as stack\n",
    "import bigfish.plot as plot\n",
    "from utils import xml_to_dataframe, dataframe_to_xml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd189a",
   "metadata": {},
   "source": [
    "Load the locations for fiducial particles in EM and LM. The locations were manually detected using ICY software and were saved in XML format. These point locations are considered to be the ground truth. For further processing we need to load the XML format and convert it into Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f3a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_xml = 'Ground_truth_fiducials_EM.xml'\n",
    "#target_xml = 'Ground_truth_fiducials_EM_only_clusters.xml'\n",
    "target_df = xml_to_dataframe(target_xml)\n",
    "print(target_df)\n",
    "\n",
    "source_xml = 'Ground_truth_fiducials_LM.xml'\n",
    "\n",
    "#source_xml = 'Ground_truth_fiducials_LM_only_clusters.xml'\n",
    "source_df_small = xml_to_dataframe(source_xml)   # small means it is smaller resolution than target, we need to scale it up\n",
    "print(source_df_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9364d2b",
   "metadata": {},
   "source": [
    "EM and LM images have most probably different sizes. LM image is usually much smaller. The points that were detected in LM image coordinate system needs to be rescaled to EM image coordinate system, otherwise it might be very hard with the registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_scale(EM_shape, LM_shape):\n",
    "    scale_x = EM_shape[0]/LM_shape[0]\n",
    "    scale_y = EM_shape[1]/LM_shape[1]\n",
    "    return scale_x, scale_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the EM and LM images\n",
    "\n",
    "input_path = \"D:/DATA/AI4Life_Pr26/20240805_Trial_data_fiducial_particles/240723_JB294_CLEM-AI4life_sample1/pos1\"\n",
    "\n",
    "EM_image_path = os.path.join(input_path, \"240726_JB295_HEK293_CLEM_LAMP1-488_Particles-555_grid4_pos1_bin4_EM.tif\")\n",
    "LM_image_path  = os.path.join(input_path, \"240726_JB295_HEK293_CLEM_LAMP1-488_Particles-555_grid4_pos1_LM.tif\")\n",
    "\n",
    "EMimage = stack.read_image(EM_image_path)\n",
    "LMimage_small = stack.read_image(LM_image_path)\n",
    "\n",
    "\n",
    "# Find what is the scaling rate between the 2 images\n",
    "scale_y, scale_x = find_the_scale(EMimage.shape, LMimage_small.shape)\n",
    "\n",
    "print(\"Scale x: \",scale_x)\n",
    "print(\"Scale y: \",scale_y)\n",
    "\n",
    "\n",
    "# Resize the LM point positions\n",
    "source_df = xml_to_dataframe(source_xml)\n",
    "source_df['pos_x'] = source_df_small['pos_x']*scale_x\n",
    "source_df['pos_y'] = source_df_small['pos_y']*scale_y\n",
    "\n",
    "\n",
    "# Resize the LM image to fit the position of the resized points\n",
    "LMimage = stack.resize_image(LMimage_small, EMimage.shape, method='bilinear')\n",
    "#EMimage_small = stack.resize_image(EMimage, LMimage_small.shape, method='bilinear')\n",
    "\n",
    "# save the resized EM image and LM image\n",
    "#output_path = \"E:/DATA/AI4Life_Pr26/20240805_Trial_data_fiducial_particles/240723_JB294_CLEM-AI4life_sample1/pos1\"\n",
    "#output_path = os.path.join(output_path, \"240726_JB295_HEK293_CLEM_LAMP1-488_Particles-555_grid4_pos1_bin4_EM_resized.tif\")\n",
    "#stack.save_image(EMimage_small, output_path)\n",
    "#output_path = os.path.join(output_path, \"240726_JB295_HEK293_CLEM_LAMP1-488_Particles-555_grid4_pos1_LM_resized.tif\")\n",
    "#stack.save_image(LMimage, output_path)\n",
    "\n",
    "#scale_x = 22.966165413533833\n",
    "#scale_y = 24.873456790123456\n",
    "print(LMimage.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58466bb",
   "metadata": {},
   "source": [
    "Converting the Panda dataframe into numpy array (2D points (X,Y) and 3D points (X,Y,Z)). The 3D points are needed if we plan to convert them to point clouds later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataframe into numpy array of coordinate pairs (X,Y) \n",
    "target = target_df[['pos_x', 'pos_y']].to_numpy()\n",
    "source = source_df[['pos_x', 'pos_y']].to_numpy()\n",
    "source_small = source_df_small[['pos_x', 'pos_y']].to_numpy()\n",
    "\n",
    "# Adding the Z-dimension to the 2D points\n",
    "target3D = np.hstack((target, np.zeros((len(target), 1))))\n",
    "source3D = np.hstack((source, np.zeros((len(source), 1))))\n",
    "source3D_small = np.hstack((source_small, np.zeros((len(source_small), 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67631d3",
   "metadata": {},
   "source": [
    "Plotting the point coordinates on top of the image to double check the correct placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5555cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot.plot_detection(LMimage_small[:,:,1], source_small, contrast=True)\n",
    "plot.plot_detection(LMimage[:,:,1], (source[:, [1, 0]]), shape=\"circle\", radius = 3*scale_y, color = \"red\", linewidth = 1, fill=False, contrast=True) \n",
    "plot.plot_detection(EMimage, (target[:, [1, 0]]), radius = 3*scale_y, contrast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c325a4",
   "metadata": {},
   "source": [
    "Converting points in numpy array (X,Y,Z) into points clouds and saving them as .ply files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to write target to a .txt file called \"target_all_2D.txt\"\n",
    "\n",
    "#np.savetxt(\"target_all_2D.txt\", target, fmt='%f', delimiter=' ')\n",
    "#np.savetxt(\"source_all_2D.txt\", source, fmt='%f', delimiter=' ')\n",
    "\n",
    "#np.savetxt(\"target_2D.txt\", target, fmt='%f', delimiter=' ')\n",
    "#np.savetxt(\"source_2D.txt\", source, fmt='%f', delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83def0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Convert NumPy array to Open3D PointCloud\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "target_pcd.points = o3d.utility.Vector3dVector(target3D)\n",
    "source_pcd = o3d.geometry.PointCloud()\n",
    "source_pcd.points = o3d.utility.Vector3dVector(source3D)\n",
    "\n",
    "# Visualize the point cloud\n",
    "#o3d.visualization.draw_geometries([target_pcd])\n",
    "#o3d.visualization.draw_geometries([source_pcd])\n",
    "\n",
    "# Assuming 'point_cloud' is your Open3D PointCloud object\n",
    "#o3d.io.write_point_cloud(\"target_all.ply\", target_pcd)\n",
    "#o3d.io.write_point_cloud(\"source_all.ply\", source_pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15cfb3",
   "metadata": {},
   "source": [
    "## Testing different registration methods provided by Probreg package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0f732",
   "metadata": {},
   "source": [
    "Define functions for visualisation of the results and plotting transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd965ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualize results\n",
    "def visualize_result_nparray(source, target, result, title):\n",
    "    source_cloud = o3d.geometry.PointCloud()\n",
    "    source_cloud.points = o3d.utility.Vector3dVector(source)\n",
    "    source_cloud.paint_uniform_color([1, 0, 0])  # Red\n",
    "\n",
    "    target_cloud = o3d.geometry.PointCloud()\n",
    "    target_cloud.points = o3d.utility.Vector3dVector(target)\n",
    "    target_cloud.paint_uniform_color([0, 1, 0])  # Green\n",
    "\n",
    "    result_cloud = o3d.geometry.PointCloud()\n",
    "    result_cloud.points = o3d.utility.Vector3dVector(result)\n",
    "    result_cloud.paint_uniform_color([0, 0, 1])  # Blue\n",
    "\n",
    "    o3d.visualization.draw_geometries([source_cloud, target_cloud, result_cloud], window_name=title)\n",
    "\n",
    "def visualize_result_pcd(source, target, result, title):\n",
    "    source.paint_uniform_color([1, 0, 0])  # Red\n",
    "    target.paint_uniform_color([0, 1, 0])  # Green\n",
    "    result.paint_uniform_color([0, 0, 1])  # Blue\n",
    "    o3d.visualization.draw_geometries([source, target, result], window_name=title)\n",
    "\n",
    "# Print transformations\n",
    "def print_transformations(transformation_paramters, title):\n",
    "    print(title)\n",
    "    print(transformation_paramters.rot)  # Rotation matrix\n",
    "    print(transformation_paramters.t)    # Translation vector\n",
    "    print(transformation_paramters.scale)  # Scale factor\n",
    "\n",
    "    # Evaluate alignment using chamfer distance\n",
    "def chamfer_distance(A, B, title):\n",
    "    distances = np.min(np.sum((A[:, np.newaxis, :] - B[np.newaxis, :, :]) ** 2, axis=2), axis=1)\n",
    "    print(f\"Chamfer distance = {np.mean(distances)} ({title}) \")\n",
    "\n",
    "    return np.mean(distances)\n",
    "\n",
    "def convert_to_pcd(nparray):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(nparray)\n",
    "    return pcd\n",
    "\n",
    "def save_correspondences_in2df(source, target, correspondences):\n",
    "    # create empty dataframe\n",
    "    source_df = pd.DataFrame()\n",
    "    target_df = pd.DataFrame()\n",
    "\n",
    "    # add columns to the dataframe ['id', 'name', 'source_pos_x', 'source_pos_y', target_pos_x, target_pos_y] and the length of the dataframe is the number of correspondences\n",
    "    source_df['id'] = range(1, len(correspondences) + 1)\n",
    "    target_df['id'] = range(1, len(correspondences) + 1)\n",
    "    source_df['name'] = \"'Point2D'\"\n",
    "    target_df['name'] = \"'Point2D'\"\n",
    "    source_df['pos_x'] = source[correspondences[:, 0], 0]\n",
    "    source_df['pos_y'] = source[correspondences[:, 0], 1]\n",
    "    target_df['pos_x'] = target[correspondences[:, 1], 0]\n",
    "    target_df['pos_y'] = target[correspondences[:, 1], 1]\n",
    "    \n",
    "    return source_df, target_df\n",
    "\n",
    "\n",
    "def save_correspondences_in1df(source, target, correspondences):\n",
    "    # create empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # add columns to the dataframe ['id', 'name', 'source_pos_x', 'source_pos_y', target_pos_x, target_pos_y] and the length of the dataframe is the number of correspondences\n",
    "    df['id'] = range(1, len(correspondences) + 1)\n",
    "    df['name'] = \"'Point2D'\"\n",
    "    df['source_pos_x'] = source[correspondences[:, 0], 0]\n",
    "    df['target_pos_x'] = target[correspondences[:, 1], 0]\n",
    "    df['source_pos_y'] = source[correspondences[:, 0], 1]\n",
    "    df['target_pos_y'] = target[correspondences[:, 1], 1]\n",
    "    df['source_ind'] = correspondences[:, 0]\n",
    "    df['target_ind'] = correspondences[:, 1]\n",
    "    df['distance'] = np.sqrt(np.sum((source[correspondences[:, 0]] - target[correspondences[:, 1]]) ** 2, axis=1))\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_correspondences(correspondences):\n",
    "    # Get unique target indices and their first occurrences\n",
    "    unique_target_indices, unique_indices = np.unique(correspondences[:, 1], return_index=True)\n",
    "\n",
    "    # Use the unique_indices to select the rows from the original array\n",
    "    cleaned_correspondences = correspondences[unique_indices]\n",
    "\n",
    "    return cleaned_correspondences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f362856",
   "metadata": {},
   "source": [
    "Loading source and target point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Load the point clouds and set the scale\n",
    "source = o3d.io.read_point_cloud(\"source.ply\") # ('source.ply') ('source_all.ply')\n",
    "target = o3d.io.read_point_cloud(\"target.ply\") # ('target.ply') ('target_all.ply')\n",
    "scale = 1000\n",
    "\n",
    "# Convert to numpy arrays and subscale the points\n",
    "source_points = np.asarray(source.points)/scale         # Subscale the points, so the physical distance between them is not too large\n",
    "target_points = np.asarray(target.points)/scale          # Subscale the points, so the physical distance between them is not too large\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c7f70",
   "metadata": {},
   "source": [
    "### Test different registration methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d967e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from probreg import callbacks\n",
    "from probreg import cpd, gmmtree, filterreg, bcpd, l2dist_regs\n",
    "\n",
    "\n",
    "# 1. Coherent Point Drift (CPD) - rigid - WORKS\n",
    "tf_param_rigid, _, _ = cpd.registration_cpd(source_points, target_points, tf_type_name='rigid', maxiter=1000, tol=1e-5)\n",
    "result_rigid = tf_param_rigid.transform(source_points)\n",
    "visualize_result_nparray(source_points, target_points, result_rigid, \"Rigid CPD\")\n",
    "\n",
    "\n",
    "# 2. Coherent Point Drift (CPD) - nonrigid - WORKS\n",
    "tf_param_nonrigid, _, _ = cpd.registration_cpd(source_points, target_points, tf_type_name='nonrigid', maxiter=1000, tol=1e-5)\n",
    "result_nonrigid = tf_param_nonrigid.transform(source_points)\n",
    "visualize_result_nparray(source_points, target_points, result_nonrigid, \"Non-rigid CPD\")\n",
    "chamfer_distance(target_points, result_nonrigid, \"Chamfer distance 1st - Nonrigid CPD\")\n",
    "\n",
    "\n",
    "# 3. Coherent Point Drift (CPD) - affine - ERROR -  LinAlgError: Singular matrix\n",
    "#tf_param_affine, _, _ = cpd.registration_cpd(source_points, target_points, tf_type_name='affine', maxiter=1000, tol=1e-5)\n",
    "#result_affine = tf_param_affine.transform(source_points)\n",
    "#visualize_result_nparray(source_points, target_points, result_affine, \"Affine CPD\")\n",
    "\n",
    "\n",
    "# 4. FilterReg - WORKS\n",
    "tf_param_filterreg, _, _ = filterreg.registration_filterreg(source_points, target_points, objective_type=\"pt2pt\")\n",
    "result_filterreg = tf_param_filterreg.transform(source_points)\n",
    "visualize_result_nparray(source_points, target_points, result_filterreg, \"FilterReg\")\n",
    "\n",
    "\n",
    "# 5. GMMReg (Gaussian Mixture Model Registration) ERROR - TypeError: cannot unpack non-iterable RigidTransformation object\n",
    "#tf_param_gmmreg, _ = l2dist_regs.registration_gmmreg(source_points, target_points)\n",
    "#result_gmmreg = tf_param_gmmreg.transform(source_points)\n",
    "#visualize_result_nparray(source_points, target_points, result_gmmreg, \"GMMReg\")\n",
    "\n",
    "\n",
    "# 6. SVR (Support Vector Registration) ERROR - InvalidParameterError: The 'gamma' parameter of OneClassSVM must be a str among {'scale', 'auto'} or a float in the range [0.0, inf). Got np.float64(inf) instead.\n",
    "#tf_param_svr = l2dist_regs.registration_svr(source_points, target_points)\n",
    "#result_svr = tf_param_svr.transform(source_points)\n",
    "#visualize_result_nparray(source_points, target_points, result_svr, \"SVR\")\n",
    "\n",
    "\n",
    "# 7. Bayesian Coherent Point Drift (BCPD) (experimental) - Error: cannot unpack non-iterable CombinedTransformation object\n",
    "#tf_param_bcpd, _ = bcpd.registration_bcpd(source_points, target_points)\n",
    "#result_bcpd = tf_param_bcpd.transform(source_points)\n",
    "#visualize_result_nparray(source_points, target_points, result_bcpd, \"BCPD\")\n",
    "\n",
    "\n",
    "# 8. GMMTree (Hierarchical Stochastic Model) - WORKS, but NO TRANSFORMATION\n",
    "#tf_param_gmmtree, _ = gmmtree.registration_gmmtree(source_points, target_points)\n",
    "#result_gmmtree = tf_param_gmmtree.transform(source_points)\n",
    "#visualize_result_nparray(source_points, target_points, result_gmmtree, \"GMMTree\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38adf54",
   "metadata": {},
   "source": [
    "Print transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "print_transformations(tf_param_rigid, \"Rigid CPD Transformation:\")\n",
    "\n",
    "# 2\n",
    "print(\"Non-rigid CPD Transformation:\")\n",
    "print(tf_param_nonrigid.g)  # Displacement field\n",
    "print(tf_param_nonrigid.w)  # Weight matrix\n",
    "\n",
    "# 3\n",
    "#print(\"Affine CPD Transformation:\")\n",
    "#print(tf_param_affine.b)  # Affine matrix\n",
    "#print(tf_param_affine.t)  # Translation vector\n",
    "\n",
    "#4\n",
    "print_transformations(tf_param_filterreg, \"FilterReg Transformation:\")\n",
    "\n",
    "#5-8\n",
    "#print_transformations(tf_param_gmmreg, \"GMMReg Transformation:\")\n",
    "#print_transformations(tf_param_svr, \"\"SVR Transformation:\"\")\n",
    "#print_transformations(result_bcpd, \"BCPD Transformation:\")\n",
    "#print_transformations(tf_param_gmmtree, \"GMMTree Transformation:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf077a8",
   "metadata": {},
   "source": [
    "### Test multilevel registration methods - first rigid and then non-rigid registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from probreg import cpd, gmmtree, filterreg, bcpd, l2dist_regs\n",
    "\n",
    "# Load the point clouds and set the scale\n",
    "source = o3d.io.read_point_cloud(\"source.ply\") # ('source.ply') ('source_all.ply')\n",
    "target = o3d.io.read_point_cloud(\"target.ply\") # ('target.ply') ('target_all.ply')\n",
    "source_all = o3d.io.read_point_cloud(\"source_all.ply\") # ('source.ply') ('source_all.ply')\n",
    "target_all = o3d.io.read_point_cloud(\"target_all.ply\") # ('target.ply') ('target_all.ply')\n",
    "scale = 1000\n",
    "\n",
    "# Convert to numpy arrays and subscale the points\n",
    "source_points = np.asarray(source.points)/scale         # Subscale the points, so the physical distance between them is not too large\n",
    "target_points = np.asarray(target.points)/scale          # Subscale the points, so the physical distance between them is not too large\n",
    "source_points_all = np.asarray(source_all.points)/scale         # Subscale the points, so the physical distance between them is not too large\n",
    "target_points_all = np.asarray(target_all.points)/scale          # Subscale the points, so the physical distance between them is not too large\n",
    "\n",
    "\n",
    "\n",
    "# 1st - registration by Coherent Point Drift (CPD) - rigid ---------------------------------------------\n",
    "tf_param_rigid, _, _ = cpd.registration_cpd(source_points, target_points, tf_type_name='rigid', maxiter=1000, tol=1e-5)\n",
    "source_points_res2 = tf_param_rigid.transform(source_points)\n",
    "source_points_all_res2 = tf_param_rigid.transform(source_points_all)\n",
    "\n",
    "visualize_result_nparray(source_points, target_points, source_points_res2, \"Rigid CPD\")\n",
    "print_transformations(tf_param_rigid, \"Rigid CPD Transformation:\")\n",
    "chamfer_distance(target_points, source_points_res2, \"Chamfer distance 1st - Rigid CPD\")\n",
    "chamfer_distance(target_points_all, source_points_all_res2, \" all Chamfer distance 2nd - Nonrigid CPD\")\n",
    "\n",
    "\n",
    "# 2nd - registration by Coherent Point Drift (CPD) - nonrigid ---------------------------------------------\n",
    "tf_param_nonrigid, _, _ = cpd.registration_cpd(source_points_res2, target_points, tf_type_name='nonrigid', maxiter=1000, tol=1e-5)\n",
    "source_points_res3 = tf_param_nonrigid.transform(source_points_res2)\n",
    "\n",
    "tf_param_nonrigid_all, _, _ = cpd.registration_cpd(source_points_all_res2, target_points_all, tf_type_name='nonrigid', maxiter=1000, tol=1e-5)\n",
    "source_points_all_res3 = tf_param_nonrigid_all.transform(source_points_all_res2)\n",
    "\n",
    "visualize_result_nparray(source_points_res2, target_points, source_points_res3, \"Nonrigid CPD\")\n",
    "\n",
    "print(\"Non-rigid CPD Transformation:\")\n",
    "print(tf_param_nonrigid.g)  # Displacement field\n",
    "print(tf_param_nonrigid.w)  # Weight matrix\n",
    "\n",
    "chamfer_distance(target_points, source_points_res3, \"Chamfer distance 2nd - Nonrigid CPD\")\n",
    "chamfer_distance(target_points_all, source_points_all_res3, \"all Chamfer distance 2nd - Nonrigid CPD\")\n",
    "\n",
    "\n",
    "\n",
    "# 3rd - finding the correspondence between the points ---------------------------------------------\n",
    "threshold = 0.02\n",
    "trans_init = np.asarray([[ 1, 0, 0, 0],  # In trying to have identity matrix as initial transformation so I can instead of source point use result of non-rigid reg points and to the ICP on those\n",
    "                         [ 0, 1, 0, 0],\n",
    "                         [ 0, 0, 1, 0],\n",
    "                         [ 0, 0, 0, 1]])\n",
    "\n",
    "sour = convert_to_pcd(source_points_res3)\n",
    "targ = convert_to_pcd(target_points)\n",
    "sour_all = convert_to_pcd(source_points_all_res3)\n",
    "targ_all = convert_to_pcd(target_points_all)\n",
    "\n",
    "evaluation = o3d.pipelines.registration.evaluate_registration(sour, targ, threshold, trans_init)\n",
    "evaluation_all = o3d.pipelines.registration.evaluate_registration(sour_all, targ_all, threshold, trans_init)\n",
    "print(\"Evaluation: \", evaluation)\n",
    "print(\"Correspondence set: \")\n",
    "print(np.asarray(evaluation.correspondence_set))\n",
    "\n",
    "print(\"Apply point-to-point ICP\")\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "    sour, targ, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "print(reg_p2p)\n",
    "print(\"Transformation is:\")\n",
    "print(reg_p2p.transformation)\n",
    "\n",
    "#evaluation = o3d.pipelines.registration.evaluate_registration(sour, targ, threshold, reg_p2p.transformation)\n",
    "#evaluation_all = o3d.pipelines.registration.evaluate_registration(sour_all, targ_all, threshold, reg_p2p.transformation)\n",
    "#print(\"Evaluation: \", evaluation)\n",
    "print(\"Correspondence set: \")\n",
    "print(np.asarray(evaluation_all.correspondence_set))\n",
    "\n",
    "\n",
    "correspondences = clean_correspondences(np.asarray(evaluation.correspondence_set))\n",
    "correspondences_all = clean_correspondences(np.asarray(evaluation_all.correspondence_set))\n",
    "print(\"Correspondences: \", correspondences_all)\n",
    "\n",
    "dff2s,dff2t = save_correspondences_in2df(source_points, target_points, correspondences)\n",
    "\n",
    "# This correspondence seems to be working in a way that corresponding point may repeat in the list. \n",
    "# same point can be corresponding to multiple points - fixed by clean_correspondences\n",
    "\n",
    "#orig_source_df = save_correspondences_in1df(np.asarray(orig_source.points), np.asarray(orig_target.points), correspondences)\n",
    "orig_source_df, orig_target_df = save_correspondences_in2df((source_points)/[scale_y,scale_x,1]*scale, \n",
    "                                                            target_points*scale, correspondences)\n",
    "\n",
    "orig_source_all_df, orig_target_all_df = save_correspondences_in2df((source_points_all)/[scale_y,scale_x,1]*scale, \n",
    "                                                            target_points_all*scale, correspondences_all)\n",
    "\n",
    "df = save_correspondences_in1df(source_points_all, target_points_all, correspondences_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f13afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the euclidean distance between the points in the orig_source_all_df and orig_target_all_df dataframe\n",
    "\n",
    "def euclidean_distance(df1, df2):\n",
    "    dist = np.sqrt((df1['pos_x'] - df2['pos_x'])**2 + (df1['pos_y'] - df2['pos_y'])**2)\n",
    "    return dist\n",
    "\n",
    "def euclidean_distance(df):\n",
    "    dist = np.sqrt((df['source_pos_x'] - df['target_pos_x'])**2 + (df['source_pos_y'] - df['target_pos_y'])**2)\n",
    "    return dist\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230da44c",
   "metadata": {},
   "source": [
    "Finding correspondences using Point2Point algorithm - This one can be applied to full set of points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b9dc4",
   "metadata": {},
   "source": [
    "Create panda dataframe with the corresponding points having the same \"id\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142cf1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_to_xml(orig_source_df, 'original_source_points_0502.xml')\n",
    "dataframe_to_xml(orig_target_df, 'original_target_points_0502.xml')\n",
    "dataframe_to_xml(orig_source_all_df, 'original_source_points_all_0502.xml')\n",
    "dataframe_to_xml(orig_target_all_df, 'original_target_points_all_0502.xml')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c367de",
   "metadata": {},
   "source": [
    "### To warp an image according to a displacement field calculated from point clouds.\n",
    "Expand the displacement field to cover the convex hull and then the entire image.\n",
    "\n",
    "To start understanding the displacement field I produce the field myself from vector between the corresponding pairs and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import flow_vis\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "\n",
    "def expand_displacement_field(points, displacements, image_shape):\n",
    "    # Create a grid covering the entire image\n",
    "    y, x = np.mgrid[0:image_shape[0], 0:image_shape[1]]\n",
    "\n",
    "    # Interpolate\n",
    "    expanded_field = griddata(points, displacements, (y, x), method='linear', fill_value=0)\n",
    " \n",
    " #   # Interpolate x and y displacements separately\n",
    " #   dx = griddata(points, displacements[:, 0] * weights, (y, x), method='linear', fill_value=0)\n",
    " #   dy = griddata(points, displacements[:, 1] * weights, (y, x), method='linear', fill_value=0)\n",
    " #   \n",
    " #   # Stack the two displacement fields\n",
    " #   expanded_field = np.stack([dx, dy], axis=-1)\n",
    "    return expanded_field\n",
    "\n",
    "def calculate_displacement_vectors(sourse_database, target_database):\n",
    "    displacements_x = np.asarray(target_database['pos_x']) - np.asarray(sourse_database['pos_x']*scale_x)\n",
    "    displacements_y = np.asarray(target_database['pos_y']) - np.asarray(sourse_database['pos_y']*scale_y)\n",
    "    displacements = np.column_stack((displacements_x, displacements_y))\n",
    "    return displacements\n",
    "\n",
    "def visualize_extended_field(expanded_field, point_cloud):\n",
    "    flow_color = flow_vis.flow_to_color(expanded_field, convert_to_bgr=False)\n",
    "    plt.imshow(flow_color)\n",
    "    plt.scatter(point_cloud[:, 1], point_cloud[:, 0], c='r', s=5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sourse_database = orig_source_df #orig_source_all_df\n",
    "target_database = orig_target_df #orig_target_all_df\n",
    "points = np.column_stack((sourse_database['pos_x']*scale_x, sourse_database['pos_y']*scale_y))\n",
    "displacements = calculate_displacement_vectors(sourse_database, target_database)\n",
    "\n",
    "expanded_field = expand_displacement_field(points, displacements, EMimage.shape)\n",
    "\n",
    "visualize_extended_field(expanded_field, points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interpn\n",
    "\n",
    "def warp_image(image, displacement_field):\n",
    "    height, width = image.shape[:2]\n",
    "    y_coords, x_coords = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
    "    \n",
    "    src_coords = np.stack([\n",
    "        y_coords + displacement_field[:,:,1], \n",
    "        x_coords + displacement_field[:,:,0]\n",
    "    ], axis=-1)\n",
    "    \n",
    "    warped_image = interpn(\n",
    "        (np.arange(height), np.arange(width)),\n",
    "        image,\n",
    "        src_coords,\n",
    "        method='linear',\n",
    "        bounds_error=False,\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    return warped_image.astype(image.dtype)\n",
    "\n",
    "warped_image = warp_image(LMimage, expanded_field)\n",
    "plt.imshow(warped_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e148d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsT = np.column_stack((target_database['pos_x'], target_database['pos_y']))\n",
    "plot.plot_detection(warped_image[:,:,1], pointsT, shape=\"circle\", radius = 3*scale_y, color = \"red\", linewidth = 1, fill=False, contrast=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aacdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_image[:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1416eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = EMimage.shape[:2]\n",
    "y_coords, x_coords = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
    "src_coords = np.stack([\n",
    "    y_coords - expanded_field[:,:,1],\n",
    "    x_coords - expanded_field[:,:,0]\n",
    "    ], axis=-1)\n",
    "#print(src_coords)\n",
    "\n",
    "src_coords[8000:8010,3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cee768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is not necessary\n",
    "# trying to use ICP as correspondence estimation, but as an input I need a pre-registered point clouds and I use rigid transformation matrix and not non-rigid one\n",
    "# https://www.open3d.org/docs/latest/tutorial/pipelines/icp_registration.html\n",
    "import copy\n",
    "\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "    \n",
    "#source = o3d.io.read_point_cloud(\"source.ply\")\n",
    "#target = o3d.io.read_point_cloud(\"target.ply\")\n",
    "#source_points, target_points, result_rigid,\n",
    "target = o3d.geometry.PointCloud()\n",
    "target.points = o3d.utility.Vector3dVector(target_points)\n",
    "source = o3d.geometry.PointCloud()\n",
    "source.points = o3d.utility.Vector3dVector(source_points_res3 )#(result_nonrigid)\n",
    "\n",
    "threshold = 0.02\n",
    "\n",
    "trans_init = np.asarray([[ 1, 0, 0, 0],  # In trying to have identity matrix as initial transformation so I can instead of source point use result of non-rigid reg points and to the ICP on those\n",
    "                         [ 0, 1, 0, 0],\n",
    "                         [ 0, 0, 1, 0],\n",
    "                         [ 0, 0, 0, 1]])\n",
    "\n",
    "\n",
    "draw_registration_result(source, target, trans_init)\n",
    "print(\"Initial alignment\")\n",
    "evaluation = o3d.pipelines.registration.evaluate_registration(\n",
    "    source, target, threshold, trans_init)\n",
    "\n",
    "print(\"Evaluation: \", evaluation)\n",
    "print(evaluation.fitness)\n",
    "print(evaluation.inlier_rmse)\n",
    "print(evaluation.correspondence_set)\n",
    "print(np.asarray(evaluation.correspondence_set))\n",
    "\n",
    "print(source.points[np.asarray(evaluation.correspondence_set[0][0])])\n",
    "print(target.points[np.asarray(evaluation.correspondence_set[0][1])])\n",
    "\n",
    "print(\"Apply point-to-point ICP\")\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "    source, target, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "print(reg_p2p)\n",
    "print(\"Transformation is:\")\n",
    "print(reg_p2p.transformation)\n",
    "draw_registration_result(source, target, reg_p2p.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9beba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load point cloud data\n",
    "orig_source = o3d.io.read_point_cloud(\"source.ply\") #(\"source_all.ply\")\n",
    "orig_target = o3d.io.read_point_cloud(\"target.ply\") #(\"target_all.ply\")\n",
    "orig_source_all = o3d.io.read_point_cloud(\"source_all.ply\")\n",
    "orig_target_all = o3d.io.read_point_cloud(\"target_all.ply\")\n",
    "\n",
    "correspondences = clean_correspondences(np.asarray(evaluation.correspondence_set))\n",
    "correspondences_all = clean_correspondences(np.asarray(evaluation_all.correspondence_set))\n",
    "\n",
    "# Upscale back the coordinates\n",
    "source_points = np.asarray(source.points)*scale\n",
    "target_points = np.asarray(target.points)*scale\n",
    "\n",
    "result_df = save_correspondences_in1df(source_points, target_points, correspondences)\n",
    "#result_source_df,result_target_df = save_correspondences_in2df(source_points, target_points, correspondences)\n",
    "print(result_df)\n",
    "#print (result_target_df)\n",
    "\n",
    "result_all_df = save_correspondences_in1df(source_points_all, target_points_all, correspondences_all)\n",
    "##############################\n",
    "\n",
    "\n",
    "#orig_source_df = save_correspondences_in1df(np.asarray(orig_source.points), np.asarray(orig_target.points), correspondences)\n",
    "orig_source_df, orig_target_df = save_correspondences_in2df(np.asarray(orig_source.points)/[scale_y,scale_x,1], \n",
    "                                                            np.asarray(orig_target.points), correspondences)\n",
    "\n",
    "orig_source_all_df, orig_target_all_df = save_correspondences_in2df(np.asarray(orig_source_all.points)/[scale_y,scale_x,1], \n",
    "                                                            np.asarray(orig_target_all.points), correspondences_all)\n",
    "\n",
    "print(orig_source_df)\n",
    "print(orig_target_df)\n",
    "print(orig_source_all_df)\n",
    "print(orig_target_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warp the source image called 'LMimage' to the target image using the estimated transformation\n",
    "matrix = np.array([[1, -0.5, 100], [0.1, 0.9, 50], [0.0015, 0.0015, 1]])\n",
    "tform = transform.ProjectiveTransform(matrix=matrix)\n",
    "tf_img = transform.warp(img, tform.inverse)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(tf_img)\n",
    "ax.set_title('Projective transformation')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after registration of source to target using cpd algorithm I want to detect the corresponding points in the source and target point clouds\n",
    "# and then calculate the distance between the corresponding points in the source and target point clouds.\n",
    "\n",
    "# I want to write the corresponding points in the source and target point clouds to a .txt file called \"corresponding_points.txt\"\n",
    "# and the distances between the corresponding points in the source and target point clouds to a .txt file called \"distances.txt\"\n",
    "\n",
    "# Convert Open3D point clouds to NumPy arrays\n",
    "source_points = np.asarray(source.points)\n",
    "target_points = np.asarray(target.points)\n",
    "result_points = np.asarray(result.points)\n",
    "\n",
    "# Find the corresponding points in the source and target point clouds\n",
    "correspondence = cpd.registration_cpd.find_correspondence(source_points, target_points, tf_param.rot, tf_param.t)\n",
    "\n",
    "# Write the corresponding points to a .txt file\n",
    "np.savetxt(\"corresponding_points.txt\", correspondence, fmt='%f', delimiter=' ')\n",
    "\n",
    "# Calculate the distances between the corresponding points in the source and target point clouds\n",
    "distances = np.linalg.norm(source_points[correspondence[:, 0]] - target_points[correspondence[:, 1]], axis=1)\n",
    "\n",
    "# Write the distances to a .txt file\n",
    "np.savetxt(\"distances.txt\", distances, fmt='%f', delimiter=' ')\n",
    "\n",
    "# Print the average distance\n",
    "print(\"Average distance:\", np.mean(distances))\n",
    "\n",
    "# I want to write the transformation matrix to a .txt file called \"transformation_matrix.txt\"\n",
    "# and the transformed source point cloud to a .ply file called \"result.ply\"\n",
    "\n",
    "# Write the transformation matrix to a .txt file\n",
    "np.savetxt(\"transformation_matrix.txt\", tf_param.rot, fmt='%f', delimiter=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1660ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after registration of source to target using cpd algorithm I want to detect the points in the source that are not in the target\n",
    "# I want to write the coordinates of these points to a .txt file called \"source_not_in_target.txt\"\n",
    "\n",
    "# Convert Open3D PointCloud to NumPy array\n",
    "source_points = np.asarray(source.points)\n",
    "target_points = np.asarray(target.points)\n",
    "result_points = np.asarray(result.points)\n",
    "\n",
    "# Find the points in the source that are not in the target\n",
    "source_not_in_target = np.setdiff1d(source_points, target_points, assume_unique=False)\n",
    "print(source_not_in_target)\n",
    "\n",
    "# Write the coordinates of these points to a .txt file\n",
    "np.savetxt(\"source_not_in_target.txt\", source_not_in_target, fmt='%f', delimiter=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transforms3d as t3d\n",
    "from probreg import filterreg\n",
    "from probreg import callbacks\n",
    "import utils\n",
    "\n",
    "\n",
    "cbs = [callbacks.Open3dVisualizerCallback(source, target)]\n",
    "tf_param, _, _ = filterreg.registration_filterreg(source, target,\n",
    "                                                  objective_type='pt2pt',\n",
    "                                                  sigma2=None,\n",
    "                                                  update_sigma2=True,\n",
    "                                                  callbacks=cbs)\n",
    "\n",
    "result = copy.deepcopy(source)\n",
    "result.points = tf_param.transform(result.points)\n",
    "\n",
    "print(\"result: \", np.rad2deg(t3d.euler.mat2euler(tf_param.rot)),\n",
    "      tf_param.scale, tf_param.t)\n",
    "print(\"result: \", tf_param.rot,\n",
    "      tf_param.scale, tf_param.t)\n",
    "# draw result\n",
    "source.paint_uniform_color([1, 0, 0])\n",
    "target.paint_uniform_color([0, 1, 0])\n",
    "result.paint_uniform_color([0, 0, 1])\n",
    "o3.visualization.draw_geometries([source, target, result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20403054",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- I was checking different registration methods\n",
    "Need to do: clean detection of fiducial particles in EM to get only the large clusters 3+ particles\n",
    "- take only the strongest points from LM image. \n",
    "- alternatively we can subsample the points first and use only a subset - hard subsample the same way both EM and LM detections\n",
    "- method for detecting registration pair and assign the same ID number to the both particles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probreg import cpd\n",
    "from probreg import callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "def prepare_source_and_target_nonrigid_2d(source_filename,\n",
    "                                          target_filename):\n",
    "    source = np.loadtxt(source_filename)\n",
    "    target = np.loadtxt(target_filename)\n",
    "    return source, target\n",
    "\n",
    "print(source)\n",
    "print(target)\n",
    "\n",
    "source, target = prepare_source_and_target_nonrigid_2d('source_2D.txt',\n",
    "                                                             'target_2D.txt')\n",
    "cbs = [callbacks.Plot2DCallback(source, target)]\n",
    "tf_param, _, _ = cpd.registration_cpd(source, target, 'affine',\n",
    "                                      callbacks=cbs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f69e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probreg import cpd\n",
    "from probreg import callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "source, target = prepare_source_and_target_nonrigid_2d('source_2D.txt',\n",
    "                                                             'target_2D.txt')\n",
    "cbs = [callbacks.Plot2DCallback(source, target)]\n",
    "tf_param, _, _ = cpd.registration_cpd(source, target, 'nonrigid',\n",
    "                                      callbacks=cbs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI4Life_OC2_2024_26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
